{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pull News Title and Paragraph Text\n",
    "\n",
    "# Base news page\n",
    "News_URL = \"https://mars.nasa.gov/news/\"\n",
    "# Pulls URL and parses to HTML using BeautifulSoup\n",
    "News_Response = requests.get(News_URL)\n",
    "News_Soup = BeautifulSoup(News_Response.text, 'html.parser')\n",
    "# Pulls the text of the title, and paragraph based on their parent div classes\n",
    "news_title = News_Soup.find(\"div\", class_=\"content_title\").text\n",
    "news_p = News_Soup.find(\"div\", class_=\"rollover_description_inner\").text\n",
    "# Test Output\n",
    "print(news_title)\n",
    "print(news_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pull largesize image from featured page\n",
    "\n",
    "# Base image page\n",
    "Small_Image_URL = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "# Pulls URL and parses to HTML using BeautifulSoup\n",
    "Small_Image_Response = requests.get(Small_Image_URL)\n",
    "Small_Image_Soup = BeautifulSoup(Small_Image_Response.text, 'html.parser')\n",
    "# Pulls the target from the \"more info\" button to get a new URL to reference the full size image\n",
    "Details_Page_URL = Small_Image_Soup.find(\"a\", class_=\"button fancybox\")[\"data-link\"]\n",
    "# Combine base with full size image URLs\n",
    "Full_Image_URL = \"https://www.jpl.nasa.gov\" + Details_Page_URL\n",
    "# Pulls NEW URL and parses to HTML using BeautifulSoup\n",
    "Full_Image_Response = requests.get(Full_Image_URL)\n",
    "Full_Image_Soup = BeautifulSoup(Full_Image_Response.text, 'html.parser')\n",
    "# Pulls tag contating full size image\n",
    "Full_Image_URL = Full_Image_Soup.find('figure', class_=\"lede\")\n",
    "# Combines base with final target URLs\n",
    "featured_image_url = \"https://www.jpl.nasa.gov\" + Full_Image_URL.a['href']\n",
    "# Test Output\n",
    "print(featured_image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pull Mars details table \n",
    "\n",
    "# Base table page\n",
    "Stats_URL = \"https://space-facts.com/mars/\"\n",
    "# Use built in Pandas function to read the webpage\n",
    "Stats_Table = pd.read_html(Stats_URL)\n",
    "# Finds first tatble and assigns it to dataframe\n",
    "Stats_Table_df = Stats_Table[0]\n",
    "# Clean format of DataFrame prior to HTML output\n",
    "Stats_Table_df.columns = [\"Mars\",\"Metrics\"]\n",
    "Stats_Table_df.reset_index(drop=True, inplace=True)\n",
    "# Output table in HTML format to be used in landing page\n",
    "Stats_html = Stats_Table_df.to_html(index=False)\n",
    "# Test Output\n",
    "Stats_Table_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# THIS KERNEL TAKES A WHILE TO FULLY PROCESS, PLEASE BE PATIENT\n",
    "# Pull all 4 enhanced image titles and URLs - \n",
    "\n",
    "\n",
    "# Base hemispheres page\n",
    "Hemisphere_Base_URL = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "# Pulls URL and parses to HTML using BeautifulSoup\n",
    "Hemisphere_Base_Response = requests.get(Hemisphere_Base_URL)\n",
    "Hemisphere_Base_Soup = BeautifulSoup(Hemisphere_Base_Response.text, 'html.parser')\n",
    "# Finds all 'item' class div tags\n",
    "Hemisphere_List = Hemisphere_Base_Soup.find_all('div', class_=\"item\")\n",
    "# Establish list variables to append to\n",
    "titles = []\n",
    "raw_img_urls = []\n",
    "full_img_urls = []\n",
    "# First loop - gets titles and URLs for enhanced photo TARGET LINKS\n",
    "for x in Hemisphere_List:\n",
    "    # Add currrently looped title to titles list\n",
    "    titles.append(x.div.text)\n",
    "    # Combine base with full size image URLs and add currrently looped URL to raw image list\n",
    "    raw_img_urls.append(\"https://astrogeology.usgs.gov\" + x.a[\"href\"])\n",
    "# Second loop - for each TARGET LINK, get the full size image URL from each of those pages\n",
    "for x in raw_img_urls:\n",
    "    # Pulls currently looped URL and parses to HTML using BeautifulSoup\n",
    "    Hemisphere_response = requests.get(x)\n",
    "    Hemisphere_Soup = BeautifulSoup(Hemisphere_response.text, 'html.parser')\n",
    "    # Pulls full size image URL from TARGET LINK\n",
    "    Hemisphere_url = Hemisphere_Soup.find(\"img\", class_=\"wide-image\")[\"src\"]\n",
    "    # Combine base URL with full image URL and add currrently looped URL to full size image list\n",
    "    full_img_urls.append(\"https://astrogeology.usgs.gov\" + Hemisphere_url)\n",
    "# Create dataframe and assign titles and full size image URLs as columns\n",
    "Hemisphere_df = pd.DataFrame()\n",
    "Hemisphere_df[\"title\"] = titles\n",
    "Hemisphere_df[\"img_url\"] = full_img_urls\n",
    "# Test Output\n",
    "Hemisphere_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data into one dictionary and output to MondoDB\n",
    "Mongo_df = Hemisphere_df.append({\"title\":news_title, \"img_url\":news_p}, ignore_index=True)\n",
    "Mongo_df = Mongo_df.append({\"title\":\"Featured Image\",\"img_url\":featured_image_url}, ignore_index=True)\n",
    "Mongo_df = Mongo_df.append({\"title\":\"Stats Table\",\"img_url\": Stats_html}, ignore_index=True)\n",
    "Mongo_dict = Mongo_df.to_dict(\"records\")\n",
    "\n",
    "# Setup connection to mongodb\n",
    "conn = \"mongodb://localhost:27017\"\n",
    "client = pymongo.MongoClient(conn)\n",
    "# Clears existing database for fresh start\n",
    "client.mars_db.web_links.drop()\n",
    "# Add all items to be passed to index.html into MongoDB\n",
    "client.mars_db.web_links.insert(Mongo_dict)\n",
    "# Test Output\n",
    "Mongo_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create single function to scrape all the data, export it to mongodb and html file\n",
    "def scrapeData():\n",
    "    # Import Dependencies\n",
    "    from bs4 import BeautifulSoup\n",
    "    import pandas as pd\n",
    "    import requests\n",
    "    import pymongo\n",
    "\n",
    "    # Base news page\n",
    "    News_URL = \"https://mars.nasa.gov/news/\"\n",
    "    # Pulls URL and parses to HTML using BeautifulSoup\n",
    "    News_Response = requests.get(News_URL)\n",
    "    News_Soup = BeautifulSoup(News_Response.text, 'html.parser')\n",
    "    # Pulls the text of the title, and paragraph based on their parent div classes\n",
    "    news_title = News_Soup.find(\"div\", class_=\"content_title\").text\n",
    "    news_p = News_Soup.find(\"div\", class_=\"rollover_description_inner\").text\n",
    "\n",
    "    # Base image page\n",
    "    Small_Image_URL = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "    # Pulls URL and parses to HTML using BeautifulSoup\n",
    "    Small_Image_Response = requests.get(Small_Image_URL)\n",
    "    Small_Image_Soup = BeautifulSoup(Small_Image_Response.text, 'html.parser')\n",
    "    # Pulls the target from the \"more info\" button to get a new URL to reference the full size image\n",
    "    Details_Page_URL = Small_Image_Soup.find(\"a\", class_=\"button fancybox\")[\"data-link\"]\n",
    "    # Combine base with full size image URLs\n",
    "    Full_Image_URL = \"https://www.jpl.nasa.gov\" + Details_Page_URL\n",
    "    # Pulls NEW URL and parses to HTML using BeautifulSoup\n",
    "    Full_Image_Response = requests.get(Full_Image_URL)\n",
    "    Full_Image_Soup = BeautifulSoup(Full_Image_Response.text, 'html.parser')\n",
    "    # Pulls tag contating full size image\n",
    "    Full_Image_URL = Full_Image_Soup.find('figure', class_=\"lede\")\n",
    "    # Combines base with final target URLs\n",
    "    featured_image_url = \"https://www.jpl.nasa.gov\" + Full_Image_URL.a['href']\n",
    "\n",
    "    # Base table page\n",
    "    Stats_URL = \"https://space-facts.com/mars/\"\n",
    "    # Use built in Pandas function to read the webpage\n",
    "    Stats_Table = pd.read_html(Stats_URL)\n",
    "    # Finds first tatble and assigns it to dataframe\n",
    "    Stats_Table_df = Stats_Table[0]\n",
    "    # Clean format of DataFrame prior to HTML output\n",
    "    Stats_Table_df.columns = [\"Mars\",\"Metrics\"]\n",
    "    Stats_Table_df.reset_index(drop=True, inplace=True)\n",
    "    # Output table in HTML format to be used in landing page\n",
    "    Stats_html = Stats_Table_df.to_html(index=False)\n",
    "\n",
    "    # Base hemispheres page\n",
    "    Hemisphere_Base_URL = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "    # Pulls URL and parses to HTML using BeautifulSoup\n",
    "    Hemisphere_Base_Response = requests.get(Hemisphere_Base_URL)\n",
    "    Hemisphere_Base_Soup = BeautifulSoup(Hemisphere_Base_Response.text, 'html.parser')\n",
    "    # Finds all 'item' class div tags\n",
    "    Hemisphere_List = Hemisphere_Base_Soup.find_all('div', class_=\"item\")\n",
    "    # Establish list variables to append to\n",
    "    titles = []\n",
    "    raw_img_urls = []\n",
    "    full_img_urls = []\n",
    "    # First loop - gets titles and URLs for enhanced photo TARGET LINKS\n",
    "    for x in Hemisphere_List:\n",
    "        # Add currrently looped title to titles list\n",
    "        titles.append(x.div.text)\n",
    "        # Combine base with full size image URLs and add currrently looped URL to raw image list\n",
    "        raw_img_urls.append(\"https://astrogeology.usgs.gov\" + x.a[\"href\"])\n",
    "    # Second loop - for each TARGET LINK, get the full size image URL from each of those pages\n",
    "    for x in raw_img_urls:\n",
    "        # Pulls currently looped URL and parses to HTML using BeautifulSoup\n",
    "        Hemisphere_response = requests.get(x)\n",
    "        Hemisphere_Soup = BeautifulSoup(Hemisphere_response.text, 'html.parser')\n",
    "        # Pulls full size image URL from TARGET LINK\n",
    "        Hemisphere_url = Hemisphere_Soup.find(\"img\", class_=\"wide-image\")[\"src\"]\n",
    "        # Combine base URL with full image URL and add currrently looped URL to full size image list\n",
    "        full_img_urls.append(\"https://astrogeology.usgs.gov\" + Hemisphere_url)\n",
    "    # Create dataframe and assign titles and full size image URLs as columns\n",
    "    Hemisphere_df = pd.DataFrame()\n",
    "    Hemisphere_df[\"title\"] = titles\n",
    "    Hemisphere_df[\"img_url\"] = full_img_urls\n",
    "\n",
    "    # Combine data into one dictionary and output to MondoDB\n",
    "    Mongo_df = Hemisphere_df.append({\"title\":news_title, \"img_url\":news_p}, ignore_index=True)\n",
    "    Mongo_df = Mongo_df.append({\"title\":\"Featured Image\",\"img_url\":featured_image_url}, ignore_index=True)\n",
    "    Mongo_df = Mongo_df.append({\"title\":\"Stats Table\",\"img_url\": Stats_html}, ignore_index=True)\n",
    "    Mongo_dict = Mongo_df.to_dict(\"records\")\n",
    "\n",
    "    # Setup connection to mongodb\n",
    "    conn = \"mongodb://localhost:27017\"\n",
    "    client = pymongo.MongoClient(conn)\n",
    "    # Clears existing database for fresh start\n",
    "    client.mars_db.web_links.drop()\n",
    "    # Add all items to be passed to index.html into MongoDB\n",
    "    client.mars_db.web_links.insert(Mongo_dict)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Output\n",
    "scrapeData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}